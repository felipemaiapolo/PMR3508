{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "analise-de-sentimentos.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felipemaiapolo/analise_sentimentos_PMR3508/blob/main/analise_de_sentimentos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK63sVgOXo_n"
      },
      "source": [
        "# Análise de sentimentos utilizando representação Doc2Vec para textos\n",
        "\n",
        "#### Disciplina: PMR3508 - Aprendizado de Máquina e Reconhecimento de Padrões\n",
        "\n",
        "Neste notebook será apresentado como fazer uso de um modelo Doc2Vec pré-treinado para a análise de sentimentos. O modelo Doc2Vec foi introduzido em 2014 por [0] e tem duas possíveis formulações, sendo que aqui focaremos naquela análoga ao Word2Vec COBW, que se chama PV-DM (Distributed Memory version of Paragraph Vector).\n",
        "\n",
        "O modelo Doc2Vec é um ótimo exemplo de como as Redes Neurais Artificiais podem ser utilizadas para gerar representações estruturadas para dados, a princípio, não estruturados. Como já foi dito, a versão mais popular do modelo Doc2Vec é uma extensão do famigerado modelo Word2Vec, que é amplamente utilizado para gerar representações para palavras e expressões. É importante dizer que os treinamentos tanto do Word2Vec quanto do Doc2Vec são dados de maneira auto supervisionada (*self-supervised*), que resumidamente é se fazer o uso de métodos supervisionados tradicionais para a realização de tarefas tradicionalmente consideradas de aprendizado não supervisionado. O aprendizado auto supervisionado é um paradigma moderno de aprendizagem e vale a pena pesquisar mais sobre.\n",
        "\n",
        "Escolhemos utilizar as representações Doc2Vec para textos neste trabalho pois, apesar de não ser o estado da arte no que tange à utilização de redes neurais para processamento de linguagem natural, temos os seguintes benefícios didáticos e práticos: (i) esse modelo é uma extensão de um dos modelos de redes neurais históricamente mais importantes para NLP, que é o Word2Vec; (ii) é um modelo relativamente simples e que dá uma boa ideia de como as Redes Neurais podem ser utilizadas para se trabalhar com dados não estruturados, e.g. textos; (iii) é um modelo de fácil/rápido treinamento e uso, o que possibilita a obtenção de um bom baseline para seus projetos.\n",
        "\n",
        "Agora faremos uma breve introdução de como utilizar um modelo Doc2Vec pré-treinado em seus projetos de Machine Learning e NLP.\n",
        "\n",
        "### Base de dados que utilizaremos\n",
        "\n",
        "Os dados utilizados neste notebook podem ser encontradas em https://ai.stanford.edu/~amaas/data/sentiment/ em sua forma bruta. Gostaríamos de agradecer aos autores de [0] por disponibilizarem os dados!\n",
        "\n",
        "\n",
        "### Referências\n",
        "\n",
        "[0] Le, Q., & Mikolov, T. (2014, January). Distributed representations of sentences and documents. In International conference on machine learning (pp. 1188-1196). Link: https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
        "\n",
        "[1] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Feab_HtqXo_o"
      },
      "source": [
        "### Iniciando..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3BVaNmWXo_q"
      },
      "source": [
        "Em primeiro lugar, vamos carregar os pacotes que faremos uso:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpwoXG01Xo_r"
      },
      "source": [
        "#Para o uso geral\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy \n",
        "import time\n",
        "from scipy.stats import uniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import io\n",
        "\n",
        "#Para o processamento de textos\n",
        "from ftfy import fix_text\n",
        "import string\n",
        "import re\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "#Para Machine Learning e NLP\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7npgg_gXo_w"
      },
      "source": [
        "Abrindo dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hODtFTa_Xo_x",
        "outputId": "1ad5117d-19ef-4541-c651-80a6c13162c2"
      },
      "source": [
        "%%time\n",
        "download = requests.get(\"https://raw.githubusercontent.com/felipemaiapolo/hands-on_ic_ml/master/data_train.csv\").content\n",
        "data = pd.read_csv(io.StringIO(download.decode('utf-8')), sep=',')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 14.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1wp8QhSXo_2",
        "outputId": "bb180975-e6f6-4b03-d341-60adc0266351"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is easily the most underrated film inn th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  positive\n",
              "0  Bromwell High is a cartoon comedy. It ran at t...         1\n",
              "1  Homelessness (or Houselessness as George Carli...         1\n",
              "2  Brilliant over-acting by Lesley Ann Warren. Be...         1\n",
              "3  This is easily the most underrated film inn th...         1\n",
              "4  This is not the typical Mel Brooks film. It wa...         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSSTMZPsXo_8"
      },
      "source": [
        "Vamos checar se há textos duplicados e especular se deveriam estar duplicados ou não:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cXcLQRnXo_9",
        "outputId": "42e414bb-7257-4c18-bb28-b3606f2b115a"
      },
      "source": [
        "data.loc[data.duplicated(subset='review', keep=False)==True].sort_values(by='review').head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24826</th>\n",
              "      <td>'Dead Letter Office' is a low-budget film abou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18434</th>\n",
              "      <td>'Dead Letter Office' is a low-budget film abou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8122</th>\n",
              "      <td>.......Playing Kaddiddlehopper, Col San Fernan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11731</th>\n",
              "      <td>.......Playing Kaddiddlehopper, Col San Fernan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21968</th>\n",
              "      <td>&lt;br /&gt;&lt;br /&gt;Back in his youth, the old man had...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21969</th>\n",
              "      <td>&lt;br /&gt;&lt;br /&gt;Back in his youth, the old man had...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8567</th>\n",
              "      <td>A have a female friend who is currently being ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8545</th>\n",
              "      <td>A have a female friend who is currently being ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5063</th>\n",
              "      <td>A longtime fan of Bette Midler, I must say her...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12112</th>\n",
              "      <td>A longtime fan of Bette Midler, I must say her...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review  positive\n",
              "24826  'Dead Letter Office' is a low-budget film abou...         0\n",
              "18434  'Dead Letter Office' is a low-budget film abou...         0\n",
              "8122   .......Playing Kaddiddlehopper, Col San Fernan...         1\n",
              "11731  .......Playing Kaddiddlehopper, Col San Fernan...         1\n",
              "21968  <br /><br />Back in his youth, the old man had...         0\n",
              "21969  <br /><br />Back in his youth, the old man had...         0\n",
              "8567   A have a female friend who is currently being ...         1\n",
              "8545   A have a female friend who is currently being ...         1\n",
              "5063   A longtime fan of Bette Midler, I must say her...         1\n",
              "12112  A longtime fan of Bette Midler, I must say her...         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z52p1OqtXpAB"
      },
      "source": [
        "Pelo fato de as avaliações duplicadas não serem muito simples, é provável que as duplicações de devam a erros na coleta dos dados e não duplicações que poderiam de fato ocorrer. Então, vamos excluir uma das avaliações duplicadas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lccjO2TXpAC",
        "outputId": "f4d7ace2-54ff-42b2-9a4d-4e6b3540bff8"
      },
      "source": [
        "data=data.drop_duplicates(subset='review', keep='first')\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24888, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tyP22QBXpAI"
      },
      "source": [
        "Temos duas colunas, sendo que a primeira contém avaliações (escritas) sobre filmes e a segunda nos diz se aquela avaliação é positiva ou negativa - se 'positive'==1 para uma certa avaliação, então aquela avaliação tem sentimento positivo. Por outro lado, se 'positive'==0 para uma certa avaliação, então aquela avaliação tem sentimento negativo. Tendo duas classes, dizemos que temos um problema de classificação binária.\n",
        "\n",
        "Vamos ver a distribuição de 'positive':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E84QLduDXpAJ",
        "outputId": "46e9d5cf-b46a-4b53-e9e8-d4619a285e0f"
      },
      "source": [
        "data.loc[:,'positive'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    12461\n",
              "0    12427\n",
              "Name: positive, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBZpNu3TXpAN"
      },
      "source": [
        "Agora vamos criar as listas $X$ e $y$, que contém os textos e os marcadores, respectivamente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6heSZUfvXpAO"
      },
      "source": [
        "X = data.loc[:,'review'].tolist()\n",
        "y = np.array(data.loc[:,'positive'].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFxxhMJYXpAi"
      },
      "source": [
        "Vamos ver um dos textos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeaAViR4XpAi",
        "outputId": "1ab6f366-c8ca-447a-e606-defb2f619bf1"
      },
      "source": [
        "X[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp1hnl-YXpAn"
      },
      "source": [
        "Assim como feito no notebook \"Introdução ao Doc2Vec\", vamos definir uma função para a limpeza e padronização dos textos.\n",
        "\n",
        "**MUITO IMPORTANTE:** é *muito* recomendado que a função de limpeza utilizada no uso do modelo de representação, i.e. Doc2Vec, seja **idêntica** à aquele utilizada no momento do treinamento do modelo de representação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKkMaUPUXpAn"
      },
      "source": [
        "def clean(text):\n",
        "    txt=text.replace(\"<br />\",\" \") #retirando tags\n",
        "    txt=fix_text(txt) #consertando Mojibakes (Ver https://pypi.org/project/ftfy/)\n",
        "    txt=txt.lower() #passando tudo para minúsculo\n",
        "    txt=txt.translate(str.maketrans('', '', string.punctuation)) #retirando toda pontuação\n",
        "    txt=txt.replace(\" — \", \" \") #retirando hífens\n",
        "    txt=re.sub(\"\\d+\", ' <number> ', txt) #colocando um token especial para os números\n",
        "    txt=re.sub(' +', ' ', txt) #deletando espaços extras\n",
        "    return txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwnmczOcXpAr"
      },
      "source": [
        "Limpando e padronizando os textos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbZdNo74XpAr",
        "outputId": "623ca951-ade8-45e3-d17f-71f3d20847f4"
      },
      "source": [
        "%%time\n",
        "X = [clean(x) for x in X]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 1min 22s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMAstVAJXpAu"
      },
      "source": [
        "Tokenizando os textos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07TTUhiAXpAv"
      },
      "source": [
        "X = [x.split() for x in X]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rFeA01XXpAz"
      },
      "source": [
        "### Utilizando Doc2Vec e modelos supervisionados para Análise de Sentimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IP5fqjlXpA0"
      },
      "source": [
        "Abrindo Doc2Vec pré-treinado (treinado no notebook \"Introdução ao Doc2Vec\"):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1hiRAfCXpA1",
        "outputId": "28c4e18c-f7a6-4395-bfe2-f4b61d6e3f8f"
      },
      "source": [
        "%%time\n",
        "d2v = Doc2Vec.load('models/doc2vec')  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 1.64 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouxO8docXpA8"
      },
      "source": [
        "Vamos definir uma função para obtermos as representações vetoriais dos textos. \n",
        "\n",
        "No momento do treinamento do Doc2Vec, geramos representações somente para os textos que foram utilizados para aquele fim. Então teremos que inferir as representações para os novos textos, que utilizaremos agora. A inferência é feita utilizando a descida pela gradiente, congelando a rede neural do Doc2Vec e atualizando somente os pesos referentes ao novos textos. Na função abaixo, fixamos uma semente (*seed*) afim de garantir resultados reprodutíveis e definimos que a descida pelo gradiente dê 20 passos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcLd0KtBXpA-"
      },
      "source": [
        "def emb(txt, model, normalize=False): \n",
        "    model.random.seed(42)\n",
        "    x=model.infer_vector(txt, steps=20)\n",
        "    \n",
        "    if normalize: return(x/np.sqrt(x@x))\n",
        "    else: return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opr0ID-dXpBJ"
      },
      "source": [
        "Obtendo as representações vetoriais para os textos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDU4wYBIXpBK",
        "outputId": "11d640d5-1dea-4990-d242-4b1eb3609996"
      },
      "source": [
        "%%time\n",
        "X = [emb(x, d2v) for x in X] \n",
        "X = np.array(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 6min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9EzMcRDXpBO"
      },
      "source": [
        "Agora que já temos $X$ e $y$ nos formatos usuais e que vocês já conhecem bem, podemos seguir para a etapa de classificação. Vamos ver os formatos dos dois arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLXLAlLQXpBP",
        "outputId": "71ff45c4-1de0-4c70-cebf-17d52097eff1"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24888, 50), (24888,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEvk0ZIRXpBS"
      },
      "source": [
        "Agora vamos treinar um modelo de Regressão Logística."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmHYJ6VCXpBS"
      },
      "source": [
        "Destinando parte da base para teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTTplIcZXpBT"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRcAf88wXpBb"
      },
      "source": [
        "Utilizando Random Search para escolher os melhores hiperparâmetros para o modelo de classificação Regressão Logística para a análise de sentimentos com base na métrica *roc_auc*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7eKr5sZXpBd",
        "outputId": "fe266560-b7f5-4b22-d209-a6a311113029"
      },
      "source": [
        "%%time\n",
        "logreg = LogisticRegression(solver='liblinear',random_state=42)\n",
        "hyperparams = dict(C=np.linspace(0,10,100), \n",
        "                     penalty=['l2', 'l1'])\n",
        "clf = RandomizedSearchCV(logreg, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\n",
        "search_logreg = clf.fit(X_train, y_train)\n",
        "\n",
        "search_logreg.best_params_, search_logreg.best_score_ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   21.5s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   35.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wall time: 36.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o--k_HENXpBi",
        "outputId": "864ae878-5fdb-4a88-9ed4-a901bac9cae6"
      },
      "source": [
        "search_logreg.best_params_, search_logreg.best_score_ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'C': 0.20202020202020202, 'penalty': 'l2'}, 0.8862058530196779)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cva784FaXpBm"
      },
      "source": [
        "Treinando modelos finais e vendo suas performances no conjunto de teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itt57i4jXpBm"
      },
      "source": [
        "logreg = LogisticRegression(C=search_logreg.best_params_['C'], \n",
        "                            penalty=search_logreg.best_params_['penalty'],\n",
        "                            solver='liblinear', random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv7_Sl0EXpBu",
        "outputId": "c833f14a-6a7d-4564-c2f4-d552180a4f74"
      },
      "source": [
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "print('AUC --- Log. Reg.: {:.4f}'.format(roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC --- Log. Reg.: 0.8791\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}